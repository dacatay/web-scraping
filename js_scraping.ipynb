{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping JavaScripts\n",
    "\n",
    "A lot of credit is owed to <a href=\"https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ\">sentdex</a> for his awesome tutorials.\n",
    "\n",
    "In this tutorial we will elarn how to scrap dynamically updated data from webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "js_test = soup.find('p', class_='jtest')\n",
    "print(js_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we are not a client, we are not a browser. What we need to do is to mimic the behavior of a browser and run the javascript hidden in the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import bs4 as bs\n",
    "\n",
    "from PyQt5.QtGui import QApplication\n",
    "from PyQt5.QtCore import QUrl\n",
    "from PyQt5.QtWebKitWidgets import QWebPage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>sys></code> module is required since <code>PyQT</code> wants access to system variables.\n",
    "\n",
    "What we will do now is create our own browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Client(QWebPage):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.app = QApplication(sys.argv)\n",
    "        QWebPage.__init__(self)\n",
    "        self.loadFinished.connect(self.on_page_load)\n",
    "        self.mainFrame().load(QUrl(url))\n",
    "        self.app.exec()\n",
    "        \n",
    "    def on_page_load(self):\n",
    "        self.app.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client <a href=\"http://www.python-course.eu/python3_inheritance.php\">inherits</a> from QWebpage. In the first step we are defining our <code>app</code> application by asigning the <code>QApplication</code> class to. We then initialize the <code>QWebpage</code>. Then, after initializing this class it we are connecting the method <code>on_page_load</code>, which we write ourselves, and execute it right after the loading is finished\n",
    "\n",
    "We are giving the <code>url</code> to Qt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://dacatay.com'\n",
    "client_response = Client(url)\n",
    "source = client_response.mainFrame().toHtml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>client_response</code> is now a <code>QWebPage</code> object and we can use it methods. We will than create our <code>soup</code> with the <code>source</code> like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = bs.BeautifulSoup(source, 'lxml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again run our test from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "js_test = soup.find('p', class_='jtest')\n",
    "print(js_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some remarks on efficiency\n",
    "\n",
    "The way we have implemented our Client above will ultimately result in two bottlenecks for us with regard to efficiency. Those being server request and response latencies as well our CPU processing.\n",
    "\n",
    "The technique above\n",
    "\n",
    "Another problem is that server request and response times may not be instant. For instance, if you are trying to crawl 100 URLs at an average latency of 300 miliseconds, the crawling will take a whopping 30 seconds, not taking into account the time to process the code.\n",
    "\n",
    " is that we are not able to utilize multi-threading. However, we willt ackle the topic of multithreading in this tutorial on how to build an automated web crawler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
